## Image Conditional Diffusion Model for Supervised Learning

### Introduction

Diffusion models have been widely used for image processing tasks such as denoising and inpainting. These models are based on the idea of propagating information from known to unknown pixels in an image, using a set of diffusion equations. The Denoising Diffusion Implicit Model (DDIM) is a recent addition to the family of diffusion models, which has been shown to achieve state-of-the-art performance on image denoising tasks.

One limitation of DDIM and other diffusion models is their inability to incorporate additional information about the image, such as the presence of certain object classes or features. In this proposal, we propose to extend DDIM by adding a conditional image input to the model, allowing it to make use of this additional information for improved performance on image segmentation tasks.

### Background

#### Diffusion Models

The Denoising Diffusion Implicit Model (DDIM) is a type of diffusion model that has been developed for image denoising tasks. It is based on the idea of propagating information from known to unknown pixels in an image, using a set of diffusion equations.

The basic principle of DDIM is to iteratively update the values of each pixel in the image based on the values of its neighbors, using a diffusion equation that incorporates both a smoothing term and a data fidelity term. The smoothing term encourages the values of adjacent pixels to be similar, while the data fidelity term ensures that the updated pixel values remain close to the original, noise-free values.

At each iteration, the DDIM model updates the values of all the pixels in the image using the following equation:
$$x_{new} = x_{old} + dt * (div(D * grad(x_{old})) + b)$$
where $x_{old}$ and $x_{new}$ are the values of the pixels at the current and next iteration, respectively; $dt$ is a time step; $D$ is a diffusion coefficient; $b$ is the noise term; $div$ and $grad$ are differential operators; and $*$ denotes element-wise multiplication.

The DDIM model can be trained to denoise an image by minimizing a loss function that measures the difference between the denoised image and the ground truth, noise-free image. The model can then be applied to denoise new images by iteratively applying the diffusion equation until convergence.

Overall, the DDIM model is able to effectively denoise images by using a combination of smoothing and data fidelity terms to propagate information from known to unknown pixels in an image.

One limitation of current diffusion models is that they are typically limited to performing tasks such as denoising and inpainting, and may not be well-suited for other image processing tasks such as image classification or segmentation. This is because diffusion models are based on the idea of propagating information from known to unknown pixels in an image, using a set of diffusion equations. While this approach can be effective for tasks such as denoising, where the goal is to smooth out noise and recover the underlying signal in an image, it may not be as effective for tasks that require more complex reasoning and decision-making, such as classifying an image or segmenting it into different regions.

Another limitation of current diffusion models is that they are typically designed to operate on static images, and may not be easily extended to video or other temporal data. This can be a disadvantage, as many real-world applications involve processing video or other temporal data, and models that are able to handle such data may have an advantage.

Finally, current diffusion models are typically limited to processing 2D images, and may not be easily extended to 3D data such as 3D images or point clouds. This can be a limitation for applications that involve 3D data, such as medical imaging or computer vision for robotics.

#### 现有图像分割模型的缺点

目前已经有一些工作将扩散模型应用到图像分割领域，尤其是医疗图像分割。
Brempong, et al. [^1] 尝试将单个Unet结构的去噪预训练模型用于通用的图像分割。
Graikos, et al. [^2] 尝试将DDPM模型优化为一个即插即用的条件模型，并尝试进行卫星图像分割。
Wolleb, et al. [^3] 同样使用DDPM模型创建一个条件扩散模型，用于医学图像分割任务。
Wu, Junde, et al. [^4] 进一步优化了扩散模型条件输入的结构，在MRI图像分割数据上得到了SOTA。
虽然目前的条件扩散模型已经展现了出色的性能，但是训练速度和扩散的步长仍然收到限制，即使是最先进的DDIM和通常只需要10步的dpm-solver，在监督学习中仍然需要大约50步。同时，条件扩散模型的成功应用仍然局限于医学图像领域，尚未在更复杂的环境图像中得到验证。对比之下，单纯的Unet结构的预训练模型已经达到了很好的预测结果。

#### Solutions

There are several possible ways to address the limitations of current diffusion models:

1.  Developing new diffusion models or variations of existing models that are better suited for tasks such as image classification or segmentation. This could involve incorporating additional information about the image or using more complex diffusion equations that are able to capture more complex relationships in the data.
2.  Developing diffusion models that are able to handle video or other temporal data, by extending the model to operate on a sequence of images or by using temporal information in the diffusion process.
3.  Developing diffusion models that are able to handle 3D data, such as 3D images or point clouds. This could involve adapting the model to operate on 3D data or by using techniques such as volumetric convolutions.
4.  Combining diffusion models with other types of models or techniques, such as convolutional neural networks (CNNs) or graph convolutional networks (GCNs), to create hybrid models that can make use of the strengths of both approaches.
5.  Developing more effective methods for incorporating additional information into diffusion models, such as using embeddings or directly inputting additional images or tensors.
6.  Improving the efficiency and scalability of diffusion models, to make them more practical for use in real-world applications.

### Objectives

The primary objective of this research is to develop an image conditional diffusion model (ICDM) for supervised learning, based on the DDIM framework. The ICDM will be trained to perform image segmentation by predicting a segmentation map for a given input image, conditioned on an additional input image that contains information about the desired object classes or features.

### Methods

To achieve this objective, we will follow the following steps:

1.  Implement the ICDM framework by modifying the DDIM model to accept an additional conditional image input.
2.  Train the ICDM on a dataset of images and corresponding segmentation maps, using the conditional image as additional input.
3.  Evaluate the performance of the ICDM on image segmentation tasks, and compare it to the performance of existing diffusion models and other state-of-the-art image segmentation methods.

### Expected results

We expect the ICDM to outperform existing diffusion models and other state-of-the-art image segmentation methods on a variety of image datasets, by making use of the additional information provided by the conditional image input. In particular, we expect the ICDM to be able to accurately segment objects of different classes or with different features, based on the information provided by the conditional image.

### Potential Influence

The research proposal titled "Image Conditional Diffusion Model for Supervised Learning" outlines a proposed study to develop an image processing model based on the Denoising Diffusion Implicit Model (DDIM) framework, with the aim of improving the performance of diffusion models on image segmentation tasks.

Image segmentation is an important problem in computer vision, as it involves dividing an image into different regions or segments based on certain characteristics or features. Accurate image segmentation can be useful for a variety of applications, such as object recognition, medical imaging, and robotics.

The proposed Image Conditional Diffusion Model (ICDM) is designed to incorporate additional information about the desired object classes or features in an image through a conditional image input, in order to improve the model's performance on image segmentation tasks. By allowing the model to make use of this additional information, it is expected that the ICDM will be able to more accurately segment objects of different classes or with different features, compared to existing diffusion models or other state-of-the-art image segmentation methods.

If the ICDM is successful in achieving these goals, it could have significant potential influence in the field of image processing and computer vision. The ability to effectively incorporate additional information into diffusion models could lead to improved performance on a variety of image processing tasks, and could potentially enable the development of new applications that rely on accurate image segmentation.

### Experience

I graduated from Xi'an Jiaotong University with a major in software engineering and am currently studying for a master's degree in computer science at the University of Sheffield.

I learned basic machine learning in the second year of undergraduate study and graph neural network (GNN) and multi-task learning (MTL) in the third year. During analyzing MTL-NAS, a model that combines network architecture search (NAS) in MTL, we found that the searched paths between tasks satisfy a specific probability distribution. The search process will degenerate into a pruning operation. Moreover, the search result will settle at the beginning of training. Therefore, we tried to add an attention mechanism to feature fusion to stable training results. Further analysis showed that attention and early stop significantly save time. In conclusion, improved MTL-NAS solved the problem of what to share and when to share with high efficiency, but each task needs a main branch resulting in a large model size.

My undergraduate graduation project focused on the handwritten digital image recognition task, which can be applied to banknote entry. I chose to predict single digits using an end-to-end image recognition model. However, datasets containing single-digit annotations are lacking in practice, so I annotated the ORAND dataset specifically for this task. Moreover, the overlapping of handwritten digits is the key issue. A three-branch model, classification branch, candidate box prediction branch, and weight prediction branch, was designed to receive the output from the backbone network. Meanwhile, overlapping predictions are filtered by Double Non-maximum Suppression (NMS), which increases recall significantly. Future implementation requires more complex preprocessing and post-processing.

Currently, I'm mainly learning natural language processing, specifically speech recognition. Last year, in conjunction with the course, I tried using fusion models for fraud detection.

In conclusion, the recognition methods I've learned and reinforcement learning are both branches of machine learning, so my knowledge will ensure that I can adapt more quickly when I'm get into reinforcement learning. At the same time, learning in multiple directions can also provide more ideas and options for future research.

### Conclusion

In this proposal, we have presented a novel approach for improving the performance of diffusion models on image segmentation tasks, by incorporating additional information about the desired object classes or features through a conditional image input. We believe that the ICDM framework has the potential to significantly advance the state of the art in image segmentation, and look forward to evaluating its performance on a variety of datasets.



[^1]: Brempong, Emmanuel Asiedu, et al. "Denoising Pretraining for Semantic Segmentation." _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2022.
[^2]: Graikos, Alexandros, et al. "Diffusion models as plug-and-play priors." _arXiv preprint arXiv:2206.09012_ (2022).
[^3]: Wolleb, Julia, et al. "Diffusion models for implicit image segmentation ensembles." _International Conference on Medical Imaging with Deep Learning_. PMLR, 2022.
[^4]: Wu, Junde, et al. "MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model." _arXiv preprint arXiv:2211.00611_ (2022).